import type { SAXStream } from 'sax';
declare class ParsingState {
    sitemapUrls: string[];
    urls: string[];
    visitedSitemapUrls: string[];
    context?: 'sitemapindex' | 'urlset';
    loc: boolean;
    resetContext(): void;
}
/**
 * Loads one or more sitemaps from given URLs, following references in sitemap index files, and exposes the contained URLs.
 *
 * **Example usage:**
 * ```javascript
 * // Load a sitemap
 * const sitemap = await Sitemap.load(['https://example.com/sitemap.xml', 'https://example.com/sitemap_2.xml.gz']);
 *
 * // Enqueue all the contained URLs (including those from sub-sitemaps from sitemap indexes)
 * await crawler.addRequests(sitemap.urls);
 * ```
 */
export declare class Sitemap {
    readonly urls: string[];
    constructor(urls: string[]);
    protected static createXmlParser(parsingState: ParsingState, onEnd: () => void, onError: (error: Error) => void): SAXStream;
    /**
     * Try to load sitemap from the most common locations - `/sitemap.xml` and `/sitemap.txt`.
     * For loading based on `Sitemap` entries in `robots.txt`, the {@apilink RobotsFile} class should be used.
     * @param url The domain URL to fetch the sitemap for.
     * @param proxyUrl A proxy to be used for fetching the sitemap file.
     */
    static tryCommonNames(url: string, proxyUrl?: string): Promise<Sitemap>;
    /**
     * Fetch sitemap content from given URL or URLs and return URLs of referenced pages.
     * @param urls sitemap URL(s)
     * @param proxyUrl URL of a proxy to be used for fetching sitemap contents
     */
    static load(urls: string | string[], proxyUrl?: string): Promise<Sitemap>;
}
export {};
//# sourceMappingURL=sitemap.d.ts.map